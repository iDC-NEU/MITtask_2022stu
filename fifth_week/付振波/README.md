# Lab4 分片KV server

## Lab4A：

**任务**：构造ShardCtrler，多个ShardCtrler组成一个Raft管理的集群，共同维护Config（配置信息）的数组。

**操作：**

- Join : 新加入的Group信息。
- Leave : 哪些Group要离开。
- Move : 将Shard分配给GID的Group,无论它原来在哪。
- Query : 查询最新的Config信息。

其实，Lab4A的任务代码思想与Lab3基本相同，不同的是，Lab4需要将DB分片，并交给不同的集群来管理，当group发生Join、Leave操作时，就涉及到负载均衡的问题。

**负载均衡：**

负载均衡的算法要求是一个确定性的算法，就是说同一个输入得到的结果也应该是相同的，我的做法是一直遍历group（从头到尾），一次给一个group分配一个shard；但是做出来之后发现这个方法过于慢，因为判断多，每次改变一个group就得重新划分一次，而且至少需要遍历NumShard（shard的数量，实验要求是10）次。这个负载均衡方法百度了一下，发现了一种多退少补的思想（这个确实很快，但是算法需要理解一下）

## Lab4B

每个组负责存储相应数量的分片，并处理这些分片的客户端请求

**Server：**

在执行客户端请求时，首先判断请求的KEY对应的分片是否由本组负责，然后进行相应的操作（返回组错误或者执行）

配置更新：Server启动时会启动一个线程负责更新配置，定期（100ms）利用Lab4A的query查询最新的配置，然后调用raft的Start接口将当前配置加入到Log中，若Log被提交，就会被发送到applyCh中，此时就会进行配置更新。

发生配置更新有几种情况：

1. 原本本组的Shard仍由本组负责——直接使用
2. 原本无人负责的shard由本组负责——创建新的分片数据
3. 原本由其他组负责的Shard：若该数据已经传输到本组，直接使用，若未传输，则创建分片，然后等待数据
4. 本组的Shard要传送给其他组：发生分片数据迁移（migrate）

分片数据迁移：

通过分片数据发送请求RPC来发送分片：在当前分片无其他任务时，将分片数据、配置号、分片号发送到目标组。

接受分片：由目标组的Leader接收（非Leader返回ErrWrongLeader），接收是将该请求加入到raft的Log中，待raft达成一致提交Log后会应用该分片。

上述的过程，存在着大量的判断，以确保接受的数据是目标数据（比如配置号是否比自己的大、分片号是否对应），这个地方对我来说是一个“坑”，总是想当然的进行几个判断，问题只有通过调试才能发现。（分布式的调试，好像只有输出中间结果的方法，可能目前学习比较少，而且是第一次使用go，以后还需努力）

